{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive label : negative label = 1.00\n",
      "The labels are balanced\n"
     ]
    }
   ],
   "source": [
    "##Question 1(a) - Parcing the txt file\n",
    "import string\n",
    "\n",
    "amazonRaw = []\n",
    "amazon = []\n",
    "imdbRaw = []\n",
    "imdb = []\n",
    "yelpRaw = []\n",
    "yelp = []\n",
    "\n",
    "amazonFile = \"amazon_cells_labelled.txt\"\n",
    "imdbFile = \"imdb_labelled.txt\"\n",
    "yelpFile = \"yelp_labelled.txt\"\n",
    "\n",
    "amazonInput = open(amazonFile).read().split('\\n')\n",
    "imdbInput = open(imdbFile).read().split('\\n')\n",
    "yelpInput = open(yelpFile).read().split('\\n')\n",
    "\n",
    "for line in amazonInput:\n",
    "    amazonRaw.append(line.split('\\t'))\n",
    "    \n",
    "for line in imdbInput:\n",
    "    imdbRaw.append(line.split('\\t'))\n",
    "\n",
    "for line in yelpInput:\n",
    "    yelpRaw.append(line.split('\\t'))\n",
    "    \n",
    "#removing neutral connotation and counting labels\n",
    "positiveLabel = 0\n",
    "negativeLabel = 0\n",
    "\n",
    "for line in amazonRaw:\n",
    "    if line[1] != \"\":\n",
    "        amazon.append(line)\n",
    "for line in amazon:\n",
    "    if line[1] == \"0\":\n",
    "        negativeLabel = negativeLabel + 1\n",
    "    else:\n",
    "        positiveLabel = positiveLabel + 1\n",
    "        \n",
    "for line in imdbRaw:\n",
    "    if line and len(line) == 2 and line[1] != \"\": #remove empty lines\n",
    "        imdb.append(line)\n",
    "for line in imdb:\n",
    "    if line[1] == \"0\":\n",
    "        negativeLabel = negativeLabel + 1\n",
    "    else:\n",
    "        positiveLabel = positiveLabel + 1\n",
    "        \n",
    "for line in yelpRaw:\n",
    "    if line[1] != \"\":\n",
    "        yelp.append(line)\n",
    "for line in yelp:\n",
    "    if line[1] == \"0\":\n",
    "        negativeLabel = negativeLabel + 1\n",
    "    else:\n",
    "        positiveLabel = positiveLabel + 1\n",
    "\n",
    "#Finding ratio of positive label to negative label        \n",
    "ratio = float(positiveLabel) / negativeLabel\n",
    "print (\"positive label : negative label = %0.2f\" %ratio)\n",
    "if ratio == 1:\n",
    "    print \"The labels are balanced\"\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Question 1 (b) - Preprocessing\n",
    "\n",
    "#to install nltk, follow instructions from http://www.nltk.org/data.html\n",
    "from nltk.stem import WordNetLemmatizer #http://textminingonline.com/dive-into-nltk-part-i-getting-started-with-nltk\n",
    "from nltk.corpus import stopwords\n",
    "#import enchant\n",
    "\n",
    "#spellcheck = enchant.Dict(\"en_US\")\n",
    "wordnet_lemmatizer = WordNetLemmatizer() #http://textminingonline.com/dive-into-nltk-part-iv-stemming-and-lemmatization\n",
    "stopwordlist = stopwords.words('english') #import stopword list from nltk\n",
    "#remove from stopword list words that might help sentiment analysis\n",
    "stopwordlist.remove('but')\n",
    "stopwordlist.remove('if')\n",
    "stopwordlist.remove('while')\n",
    "stopwordlist.remove('once')\n",
    "stopwordlist.remove('no')\n",
    "stopwordlist.remove('not')\n",
    "stopwordlist.remove('nor') \n",
    "\n",
    "preAmazon = []\n",
    "preImdb = []\n",
    "preYelp = []\n",
    "#feature vectors in bags of words only care about unique words. These pre-processing aims to reduce noise or repeating words\n",
    "\n",
    "for line in amazon:\n",
    "    line[0] = line[0].translate(string.maketrans(\"\",\"\"), string.punctuation) #remove punctuation\n",
    "    line[0] = line[0].strip().lower() #lowercase all words\n",
    "    line[0] = line[0].split() #spliting string into list of words to enable easy word by word access\n",
    "    #line[0] = list(set(line[0]) - set(stopwordlist)) #stripping stop words\n",
    "    s = set(stopwordlist)\n",
    "    line[0] = [x for x in line[0] if x not in s]\n",
    "               \n",
    "    for word in line[0]:\n",
    "        ##perform spell check to ensure accurate feature vector extraction and reduce noise\n",
    "        #if spellcheck.check(word) == False:\n",
    "        #    line[0].remove()\n",
    "    \n",
    "        #word = LancasterStemmer.stem(word) #stemming (find root word)\n",
    "        word = wordnet_lemmatizer.lemmatize(word.decode('utf8')) #lematizing to find root words, more accurate than stemming \n",
    "    preAmazon.append(line)\n",
    "    \n",
    "for line in imdb:\n",
    "    line[0] = line[0].translate(string.maketrans(\"\",\"\"), string.punctuation) #remove punctuation\n",
    "    line[0] = line[0].strip().lower() #lowercase all words\n",
    "    line[0] = line[0].split() #spliting string into list of words to enable easy word by word access\n",
    "    line[0] = list(set(line[0]) - set(stopwordlist)) #stripping stop words\n",
    "    \n",
    "    for word in line[0]:\n",
    "        word = wordnet_lemmatizer.lemmatize(word.decode('utf8')) #lematizing to find root words, more accurate than stemming \n",
    "    preImdb.append(line)\n",
    "    \n",
    "for line in yelp:\n",
    "    line[0] = line[0].translate(string.maketrans(\"\",\"\"), string.punctuation) #remove punctuation\n",
    "    line[0] = line[0].strip().lower() #lowercase all words\n",
    "    line[0] = line[0].split() #spliting string into list of words to enable easy word by word access\n",
    "    line[0] = list(set(line[0]) - set(stopwordlist)) #stripping stop words\n",
    "    \n",
    "    for word in line[0]:\n",
    "        word = wordnet_lemmatizer.lemmatize(word.decode('utf8')) #lematizing to find root words, more accurate than stemming \n",
    "    preYelp.append(line)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Question 1(c) - Splitting data into training and testing set\n",
    "\n",
    "train = []\n",
    "test = []\n",
    "trainLabel = []\n",
    "testLabel = []\n",
    "posCount = 0\n",
    "negCount = 0\n",
    "\n",
    "for line in preAmazon:\n",
    "    if line[1] == \"0\":\n",
    "        negCount = negCount +1\n",
    "        if negCount <= 400:\n",
    "            train.append(line[0])\n",
    "            trainLabel.append(line[1])\n",
    "        else:\n",
    "            test.append(line[0])\n",
    "            testLabel.append(line[1])\n",
    "    elif line[1] == \"1\":\n",
    "        posCount = posCount +1\n",
    "        if posCount <= 400:\n",
    "            train.append(line[0])\n",
    "            trainLabel.append(line[1])\n",
    "        else:\n",
    "            test.append(line[0])\n",
    "            testLabel.append(line[1])\n",
    "\n",
    "posCount = 0\n",
    "negCount = 0\n",
    "\n",
    "for line in preImdb:\n",
    "    if line[1] == \"0\":\n",
    "        negCount = negCount +1\n",
    "        if negCount <= 400:\n",
    "            train.append(line[0])\n",
    "            trainLabel.append(line[1])\n",
    "        else:\n",
    "            test.append(line[0])\n",
    "            testLabel.append(line[1])\n",
    "    elif line[1] == \"1\":\n",
    "        posCount = posCount +1\n",
    "        if posCount <= 400:\n",
    "            train.append(line[0])\n",
    "            trainLabel.append(line[1])\n",
    "        else:\n",
    "            test.append(line[0])\n",
    "            testLabel.append(line[1])\n",
    "\n",
    "posCount = 0\n",
    "negCount = 0\n",
    "\n",
    "for line in preYelp:\n",
    "    if line[1] == \"0\":\n",
    "        negCount = negCount +1\n",
    "        if negCount <= 400:\n",
    "            train.append(line[0])\n",
    "            trainLabel.append(line[1])\n",
    "        else:\n",
    "            test.append(line[0])\n",
    "            testLabel.append(line[1])\n",
    "    elif line[1] == \"1\":\n",
    "        posCount = posCount +1\n",
    "        if posCount <= 400:\n",
    "            train.append(line[0])\n",
    "            trainLabel.append(line[1])\n",
    "        else:\n",
    "            test.append(line[0])\n",
    "            testLabel.append(line[1])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Question 1(d) - Creating Bag of Words Model\n",
    "from collections import Counter\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "features = []\n",
    "trainFeatures = []\n",
    "testFeatures = []\n",
    "#creating unique word list\n",
    "for line in train:\n",
    "    line = list(set(line) - set(features))\n",
    "    for word in line:\n",
    "        features.append(word)\n",
    "\n",
    "#create feature vector for each review - representing word frequency\n",
    "for line in train:\n",
    "    array = []\n",
    "    dictionary = Counter(line)\n",
    "    for feature in features:\n",
    "        array.append(dictionary[feature])\n",
    "    trainFeatures.append(array)\n",
    "\n",
    "for line in test:\n",
    "    array = []\n",
    "    dictionary = Counter(line)\n",
    "    for feature in features:\n",
    "        array.append(dictionary[feature])\n",
    "    testFeatures.append(array)\n",
    "\n",
    "example1 = random.choice(trainFeatures)\n",
    "example2 = random.choice(trainFeatures)\n",
    "\n",
    "while example1 == example2:\n",
    "    example2 = random.choice(trainFeatures)\n",
    "    \n",
    "plt.plot(example1)\n",
    "plt.suptitle('Feature Vector of a Review in Training Set', fontsize=20)\n",
    "plt.xlabel('element', fontsize=16)\n",
    "plt.ylabel('frequency', fontsize=16)\n",
    "plt . savefig (\"sample_review1.png\") #saving graph as .png file\n",
    "plt.close()\n",
    "\n",
    "plt.plot(example2)\n",
    "plt.suptitle('Feature Vector of a Review in Training Set', fontsize=20)\n",
    "plt.xlabel('element', fontsize=16)\n",
    "plt.ylabel('frequency', fontsize=16)\n",
    "plt . savefig (\"sample_review2.png\") #saving graph as .png file\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Question 1(e) - postprocessing\n",
    "\n",
    "from numpy import linalg as LA\n",
    "\n",
    "#normalizing by l2 to reduce variance\n",
    "for i in range(len(trainFeatures)):\n",
    "    norm2 = LA.norm(trainFeatures[i],2)\n",
    "    for j in range(len(trainFeatures[i])):\n",
    "        trainFeatures[i][j] /= norm2\n",
    "\n",
    "for i in range(len(testFeatures)):\n",
    "    norm2 = LA.norm(testFeatures[i],2)\n",
    "    for j in range(len(testFeatures[i])):\n",
    "        if norm2 == 0.0:\n",
    "            testFeatures[i][j] = 0.0\n",
    "        else:\n",
    "            testFeatures[i][j] /= norm2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Means Clustering classification accuracy: 0.54\n"
     ]
    }
   ],
   "source": [
    "#Question 1(f) - K-means clustering\n",
    "from scipy.spatial import distance\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "#initialise cluster centers\n",
    "C1_new = random.choice(trainFeatures)\n",
    "C2_new = random.choice(trainFeatures)\n",
    "while C1_new == C2_new:\n",
    "    C2_new = random.choice(trainFeatures)\n",
    "\n",
    "C1_old = np.zeros(len(C1_new))\n",
    "C2_old = np.zeros(len(C2_new))\n",
    "C1_new = np.array(C1_new)\n",
    "C1_new = np.array(C1_new)\n",
    "\n",
    "while np.mean(C1_new - C1_old) != 0.0 and np.mean(C2_new - C2_old) != 0.0:\n",
    "    cluster1 = [] #array for all elements in cluster1\n",
    "    cluster2 = [] #array for all elements in cluster2\n",
    "    cluster1.append(C1_new)\n",
    "    cluster2.append(C2_new)\n",
    "    for line in trainFeatures:\n",
    "        distance1 = distance.euclidean(line,C1_new)\n",
    "        distance2 = distance.euclidean(line,C2_new)\n",
    "        if distance1 < distance2: #review belong to cluster centre 1\n",
    "            cluster1.append(line)\n",
    "        else: \n",
    "            cluster2.append(line)\n",
    "    C1_old = C1_new\n",
    "    C2_old = C2_new\n",
    "    C1_new = np.mean(cluster1, axis = 0)\n",
    "    C2_new = np.mean(cluster2, axis = 0)\n",
    "    \n",
    "##plotting histogram\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(C1_new)\n",
    "plt.suptitle('Cluster1 - normal', fontsize=20)\n",
    "plt.xlabel('element', fontsize=16)\n",
    "plt.ylabel('frequency', fontsize=16)\n",
    "plt . savefig (\"Cluster1 - normal.png\") #saving graph as .png file\n",
    "plt.close()\n",
    "\n",
    "plt.plot(C2_new)\n",
    "plt.suptitle('Cluster2 - normal', fontsize=20)\n",
    "plt.xlabel('element', fontsize=16)\n",
    "plt.ylabel('frequency', fontsize=16)\n",
    "plt . savefig (\"Cluster2 - normal.png\") #saving graph as .png file\n",
    "plt.close()\n",
    "\n",
    "\n",
    "\n",
    "#finding Classification Accuracy\n",
    "kTrainLabel = []\n",
    "\n",
    "for line in trainFeatures:\n",
    "    distance1 = distance.euclidean(line,C1_new)\n",
    "    distance2 = distance.euclidean(line,C2_new)\n",
    "    if distance1 > distance2:\n",
    "        kTrainLabel.append(\"0\")\n",
    "    else:\n",
    "        kTrainLabel.append(\"1\")\n",
    "\n",
    "kTrainLabel= np.array(kTrainLabel)\n",
    "trainLabel = np.array(trainLabel)\n",
    "accuracy = np.sum(kTrainLabel == trainLabel)/float(len(trainLabel))\n",
    "\n",
    "print (\"K-Means Clustering classification accuracy: %0.2f\" %accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression classification accuracy: 0.81\n",
      "[[230, 70], [44, 256]]\n",
      "great\n",
      "good\n",
      "love\n",
      "excellent\n",
      "nice\n",
      "delicious\n",
      "best\n",
      "loved\n",
      "fantastic\n",
      "works\n"
     ]
    }
   ],
   "source": [
    "#Question 1(g) - Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "        \n",
    "#performing Logistic Regression\n",
    "model = LogisticRegression()\n",
    "trainFeatures = np.array(trainFeatures)\n",
    "testFeatures = np.array(testFeatures)\n",
    "trainLabel = np.array(trainLabel)\n",
    "testLabel = np.array(testLabel)\n",
    "\n",
    "#predicting classification accuracy\n",
    "model.fit(trainFeatures,trainLabel)\n",
    "predicted = model.predict(testFeatures)\n",
    "accuracy = np.sum(predicted == testLabel) / float(len(testLabel))\n",
    "\n",
    "#constructing confusion matrix\n",
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "\n",
    "for i in range(len(testLabel)):\n",
    "    if predicted[i] == \"0\" and testLabel[i] == \"0\":\n",
    "        TN += 1\n",
    "    elif predicted[i] == \"0\" and testLabel[i] == \"1\":\n",
    "        FN += 1\n",
    "    elif predicted[i] == \"1\" and testLabel[i] == \"1\":\n",
    "        TP += 1\n",
    "    else:\n",
    "        FP += 1\n",
    "        \n",
    "confMatrix = [[TP, FN], [FP, TN]]\n",
    "\n",
    "print (\"Logistic Regression classification accuracy: %0.2f\" %accuracy)\n",
    "print confMatrix\n",
    "\n",
    "#return top 10 words which are most important\n",
    "import heapq\n",
    "weightVector = model.coef_\n",
    "weightVector = weightVector.tolist()\n",
    "\n",
    "coefficient = heapq.nlargest(10, weightVector[0])\n",
    "weightedWords = []\n",
    "for line in coefficient:\n",
    "    weightedWords.append(features[weightVector[0].index(line)])\n",
    "for line in weightedWords:\n",
    "    print line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Question 1(h) - repeat (d) to (g) using 2-gram model\n",
    "\n",
    "#(d) - Creating Bag of Words Model\n",
    "\n",
    "gramFeatures = []\n",
    "gramTrain = []\n",
    "gramTest = []\n",
    "gramTrainFeatures = []\n",
    "gramTestFeatures = []\n",
    "\n",
    "#create 2-gram training data\n",
    "for i in range(len(train)):\n",
    "    array = []\n",
    "    for j in range(len(train[i])):\n",
    "        if j+1 == len(train[i]):\n",
    "            break\n",
    "        else:\n",
    "            array.append(train[i][j] + \" \" + train[i][j+1])\n",
    "    gramTrain.append(array)\n",
    "\n",
    "#create 2-gram testing data\n",
    "for i in range(len(test)):\n",
    "    array = []\n",
    "    for j in range(len(test[i])):\n",
    "        if j+1 == len(test[i]):\n",
    "            break\n",
    "        else:\n",
    "            array.append(test[i][j] + \" \" + test[i][j+1])\n",
    "    gramTest.append(array)\n",
    "\n",
    "#creating unique word list\n",
    "for line in gramTrain:\n",
    "    line = list(set(line) - set(gramFeatures))\n",
    "    for word in line:\n",
    "        gramFeatures.append(word)\n",
    "        \n",
    "    \n",
    "#for i in range(len(gramTrain)):\n",
    "#    train[i] = list(set(gramTrain[i]) - set(gramFeatures))\n",
    "#    for j in range(len(train[i])):\n",
    "#        if j+1 == len(train[i]):\n",
    "#            break\n",
    "#        else:\n",
    "#            gramFeatures.append([train[i][j] + \" \" + train[i][j+1]])\n",
    "\n",
    "#create feature vector for each review - representing word frequency\n",
    "for line in gramTrain:\n",
    "    array = []\n",
    "    dictionary = Counter(line)\n",
    "    for feature in gramFeatures:\n",
    "        array.append(dictionary[feature])\n",
    "    gramTrainFeatures.append(array)\n",
    "\n",
    "for line in gramTest:\n",
    "    array = []\n",
    "    dictionary = Counter(line)\n",
    "    for feature in gramFeatures:\n",
    "        array.append(dictionary[feature])\n",
    "    gramTestFeatures.append(array)\n",
    "\n",
    "#(e) - postprocessing\n",
    "#normalizing by l2 to reduce variance\n",
    "for i in range(len(gramTrainFeatures)):\n",
    "    norm2 = LA.norm(gramTrainFeatures[i],2)\n",
    "    for j in range(len(gramTrainFeatures[i])):\n",
    "        if norm2 == 0.0:\n",
    "            gramTrainFeatures[i][j] = 0.0\n",
    "        else:\n",
    "            gramTrainFeatures[i][j] /= norm2\n",
    "\n",
    "for i in range(len(gramTestFeatures)):\n",
    "    norm2 = LA.norm(gramTestFeatures[i],2)\n",
    "    for j in range(len(gramTestFeatures[i])):\n",
    "        if norm2 == 0.0:\n",
    "            gramTestFeatures[i][j] = 0.0\n",
    "        else:\n",
    "            gramTestFeatures[i][j] /= norm2\n",
    "\n",
    "#(f) - K-means clustering\n",
    "#initialise cluster centers\n",
    "C1_new = random.choice(gramTrainFeatures)\n",
    "C2_new = random.choice(gramTrainFeatures)\n",
    "while C1_new == C2_new:\n",
    "    C2_new = random.choice(gramTrainFeatures) #prevent same cluster centres from being chosen for both C1 and C2\n",
    "\n",
    "C1_old = np.zeros(len(C1_new))\n",
    "C2_old = np.zeros(len(C2_new))\n",
    "C1_new = np.array(C1_new)\n",
    "C1_new = np.array(C1_new)\n",
    "\n",
    "while np.mean(C1_new - C1_old) != 0.0 and np.mean(C2_new - C2_old) != 0.0:\n",
    "    cluster1 = [] #array for all elements in cluster1\n",
    "    cluster2 = [] #array for all elements in cluster2\n",
    "    cluster1.append(C1_new)\n",
    "    cluster2.append(C2_new)\n",
    "    for line in gramTrainFeatures:\n",
    "        distance1 = distance.euclidean(line,C1_new)\n",
    "        distance2 = distance.euclidean(line,C2_new)\n",
    "        if distance1 > distance2: #review belong to cluster centre 1\n",
    "            cluster1.append(line)\n",
    "        else: \n",
    "            cluster2.append(line)\n",
    "    C1_old = C1_new\n",
    "    C2_old = C2_new\n",
    "    C1_new = np.mean(cluster1, axis = 0)\n",
    "    C2_new = np.mean(cluster2, axis = 0)\n",
    "    \n",
    "##plotting histogram\n",
    "plt.plot(C1_new)\n",
    "plt.suptitle('Cluster1 - 2gram', fontsize=20)\n",
    "plt.xlabel('element', fontsize=16)\n",
    "plt.ylabel('frequency', fontsize=16)\n",
    "plt . savefig (\"Cluster1 - 2gram.png\") #saving graph as .png file\n",
    "plt.close()\n",
    "\n",
    "plt.plot(C2_new)\n",
    "plt.suptitle('Cluster2 - 2gram', fontsize=20)\n",
    "plt.xlabel('element', fontsize=16)\n",
    "plt.ylabel('frequency', fontsize=16)\n",
    "plt . savefig (\"Cluster2 - 2gram.png\") #saving graph as .png file\n",
    "plt.close()\n",
    "\n",
    "#finding Classification Accuracy\n",
    "kGramTrainLabel = []\n",
    "\n",
    "for line in gramTrainFeatures:\n",
    "    distance1 = distance.euclidean(line,C1_new)\n",
    "    distance2 = distance.euclidean(line,C2_new)\n",
    "    if distance1 < distance2:\n",
    "        kGramTrainLabel.append(\"0\")\n",
    "    else:\n",
    "        kGramTrainLabel.append(\"1\")\n",
    "\n",
    "kGramTrainLabel= np.array(kGramTrainLabel)\n",
    "trainLabel = np.array(trainLabel)\n",
    "accuracy = np.sum(kGramTrainLabel == trainLabel)/float(len(trainLabel))\n",
    "\n",
    "print (\"K-Means Clustering classification accuracy for 2-gram model: %0.2f\" %accuracy)\n",
    "\n",
    "#(g) - Logistic Regression\n",
    "        \n",
    "#performing Logistic Regression\n",
    "model = LogisticRegression()\n",
    "gramTrainFeatures = np.array(gramTrainFeatures)\n",
    "gramTestFeatures = np.array(gramTestFeatures)\n",
    "testLabel = np.array(testLabel)\n",
    "\n",
    "#predicting classification accuracy\n",
    "model.fit(gramTrainFeatures,trainLabel)\n",
    "predicted = model.predict(gramTestFeatures)\n",
    "accuracy = np.sum(predicted == testLabel) / float(len(testLabel))\n",
    "\n",
    "#constructing confusion matrix\n",
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "\n",
    "for i in range(len(testLabel)):\n",
    "    if predicted[i] == \"0\" and testLabel[i] == \"0\":\n",
    "        TN += 1\n",
    "    elif predicted[i] == \"0\" and testLabel[i] == \"1\":\n",
    "        FN += 1\n",
    "    elif predicted[i] == \"1\" and testLabel[i] == \"1\":\n",
    "        TP += 1\n",
    "    else:\n",
    "        FP += 1\n",
    "        \n",
    "confMatrix = [[TP, FN], [FP, TN]]\n",
    "\n",
    "print (\"Logistic Regression classification accuracy: %0.2f\" %accuracy)\n",
    "print confMatrix\n",
    "\n",
    "#return top 10 words which are most important\n",
    "weightVector = model.coef_\n",
    "weightVector = weightVector.tolist()\n",
    "\n",
    "coefficient = heapq.nlargest(10, weightVector[0])\n",
    "weightedWords = []\n",
    "print len(coefficient)\n",
    "for line in coefficient:\n",
    "    weightedWords.append(gramFeatures[weightVector[0].index(line)])\n",
    "for line in weightedWords:\n",
    "    print line\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression classification accuracy: 0.64\n",
      "[[206, 94], [121, 179]]\n",
      "great\n",
      "great\n",
      "not\n",
      "service\n",
      "service\n",
      "time\n",
      "food\n",
      "movie\n",
      "place\n",
      "film\n",
      "Logistic Regression classification accuracy: 0.70\n",
      "[[185, 115], [67, 233]]\n",
      "great\n",
      "great\n",
      "not\n",
      "service\n",
      "service\n",
      "time\n",
      "food\n",
      "movie\n",
      "place\n",
      "film\n",
      "Logistic Regression classification accuracy: 0.76\n",
      "[[213, 87], [55, 245]]\n",
      "great\n",
      "great\n",
      "not\n",
      "service\n",
      "service\n",
      "time\n",
      "food\n",
      "movie\n",
      "place\n",
      "film\n"
     ]
    }
   ],
   "source": [
    "#Question 1(j) - Principal Component Analysis (http://sebastianraschka.com/Articles/2014_pca_step_by_step.html)\n",
    "\n",
    "#PERFORMING PCA\n",
    "#averaging trainFeatures feature vectors\n",
    "averageTrain = np.mean(trainFeatures, axis = 0)\n",
    "\n",
    "subtractedTrain, subtractedTest = [], []\n",
    "for line in trainFeatures:\n",
    "    subtractedTrain.append(line - averageTrain)\n",
    "\n",
    "for line in testFeatures:\n",
    "    subtractedTest.append(line - averageTrain)\n",
    "\n",
    "subtractedTrain, subtractedTest = np.array(subtractedTrain, dtype=float), np.array(subtractedTest, dtype=float)\n",
    "\n",
    "#performing SVD on subtracted data and choosing n largest eigenvectors and eigenvalues\n",
    "U, S, V = np.linalg.svd(subtractedTrain, full_matrices = False)\n",
    "\n",
    "eig_vec_10 = []\n",
    "eig_vec_50 = []\n",
    "eig_vec_100 = []\n",
    "\n",
    "for i in range(10):\n",
    "    eig_vec_10.append(V[i,:])\n",
    "\n",
    "for i in range(50):\n",
    "    eig_vec_50.append(V[i,:])\n",
    "    \n",
    "for i in range(100):\n",
    "    eig_vec_100.append(V[i,:])\n",
    "\n",
    "eig_vec_10 = np.array(eig_vec_10)\n",
    "eig_vec_50 = np.array(eig_vec_50)\n",
    "eig_vec_100 = np.array(eig_vec_100)\n",
    "\n",
    "#getting training data with reduced dimensions\n",
    "transformed_10 = subtractedTrain.dot(eig_vec_10.T)\n",
    "transformed_50 = subtractedTrain.dot(eig_vec_50.T)\n",
    "transformed_100 = subtractedTrain.dot(eig_vec_100.T)\n",
    "\n",
    "transformed_test_10 = subtractedTest.dot(eig_vec_10.T)\n",
    "transformed_test_50 = subtractedTest.dot(eig_vec_50.T)\n",
    "transformed_test_100 = subtractedTest.dot(eig_vec_100.T)\n",
    "\n",
    "#(f) - K-means clustering - 10\n",
    "#initialise cluster centers\n",
    "C1_new = random.choice(transformed_10)\n",
    "C2_new = random.choice(transformed_10)\n",
    "while np.mean(C1_new - C2_new) == 0.0:\n",
    "    C2_new = random.choice(transformed_10) #prevent same cluster centres from being chosen for both C1 and C2\n",
    "\n",
    "C1_old = np.zeros(len(C1_new))\n",
    "C2_old = np.zeros(len(C2_new))\n",
    "C1_new = np.array(C1_new)\n",
    "C1_new = np.array(C1_new)\n",
    "\n",
    "while np.mean(C1_new - C1_old) != 0.0 and np.mean(C2_new - C2_old) != 0.0:\n",
    "    cluster1 = [] #array for all elements in cluster1\n",
    "    cluster2 = [] #array for all elements in cluster2\n",
    "    cluster1.append(C1_new)\n",
    "    cluster2.append(C2_new)\n",
    "    for line in transformed_10:\n",
    "        distance1 = distance.euclidean(line,C1_new)\n",
    "        distance2 = distance.euclidean(line,C2_new)\n",
    "        if distance1 > distance2: #review belong to cluster centre 1\n",
    "            cluster1.append(line)\n",
    "        else: \n",
    "            cluster2.append(line)\n",
    "    C1_old = C1_new\n",
    "    C2_old = C2_new\n",
    "    C1_new = np.mean(cluster1, axis = 0)\n",
    "    C2_new = np.mean(cluster2, axis = 0)\n",
    "    \n",
    "##plotting histogram\n",
    "#import matplotlib.pyplot as plt\n",
    "plt.plot(C1_new)\n",
    "plt.suptitle('Cluster1 - pca_10D', fontsize=20)\n",
    "plt.xlabel('element', fontsize=16)\n",
    "plt.ylabel('frequency', fontsize=16)\n",
    "plt . savefig (\"Cluster1 - pca_10D.png\") #saving graph as .png file\n",
    "plt.close()\n",
    "\n",
    "plt.plot(C2_new)\n",
    "plt.suptitle('Cluster2 - pca_10D', fontsize=20)\n",
    "plt.xlabel('element', fontsize=16)\n",
    "plt.ylabel('frequency', fontsize=16)\n",
    "plt . savefig (\"Cluster2 - pca_10D.png\") #saving graph as .png file\n",
    "plt.close()\n",
    "\n",
    "#finding Classification Accuracy\n",
    "kTrainLabel_10 = []\n",
    "\n",
    "for line in transformed_10:\n",
    "    distance1 = distance.euclidean(line,C1_new)\n",
    "    distance2 = distance.euclidean(line,C2_new)\n",
    "    if distance1 < distance2:\n",
    "        kTrainLabel_10.append(\"0\")\n",
    "    else:\n",
    "        kTrainLabel_10.append(\"1\")\n",
    "\n",
    "kTrainLabel_10= np.array(kTrainLabel_10)\n",
    "trainLabel = np.array(trainLabel)\n",
    "accuracy = np.sum(kTrainLabel_10 == trainLabel)/float(len(trainLabel))\n",
    "\n",
    "print (\"K-Means Clustering classification accuracy for 10-D: %0.2f\" %accuracy)\n",
    "\n",
    "#(g) - Logistic Regression\n",
    "        \n",
    "#performing Logistic Regression\n",
    "model = LogisticRegression()\n",
    "transformed_10 = np.array(transformed_10)\n",
    "transformed_test_10 = np.array(transformed_test_10)\n",
    "testLabel = np.array(testLabel)\n",
    "\n",
    "#predicting classification accuracy\n",
    "model.fit(transformed_10,trainLabel)\n",
    "predicted = model.predict(transformed_test_10)\n",
    "accuracy = np.sum(predicted == testLabel) / float(len(testLabel))\n",
    "\n",
    "#constructing confusion matrix\n",
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "\n",
    "for i in range(len(testLabel)):\n",
    "    if predicted[i] == \"0\" and testLabel[i] == \"0\":\n",
    "        TN += 1\n",
    "    elif predicted[i] == \"0\" and testLabel[i] == \"1\":\n",
    "        FN += 1\n",
    "    elif predicted[i] == \"1\" and testLabel[i] == \"1\":\n",
    "        TP += 1\n",
    "    else:\n",
    "        FP += 1\n",
    "        \n",
    "confMatrix = [[TP, FN], [FP, TN]]\n",
    "\n",
    "print (\"Logistic Regression classification accuracy: %0.2f\" %accuracy)\n",
    "print confMatrix\n",
    "\n",
    "#return top 10 words which are most important\n",
    "array = []\n",
    "\n",
    "for i in range(10):\n",
    "    x = V[i,:]\n",
    "    x = x.tolist()\n",
    "    xIndex = x.index(max(x))\n",
    "    array.append(features[xIndex])\n",
    "                             \n",
    "for line in array:\n",
    "    print line\n",
    "#weightVector = model.coef_\n",
    "#weightVector = weightVector.tolist()\n",
    "\n",
    "#coefficient = heapq.nlargest(10, weightVector[0])\n",
    "#weightedWords = []\n",
    "\n",
    "#for line in coefficient:\n",
    "#    weightedWords.append(features[weightVector[0].index(line)])\n",
    "#for line in weightedWords:\n",
    "#    print line\n",
    "    \n",
    "#(f) - K-means clustering - 50\n",
    "#initialise cluster centers\n",
    "C1_new = random.choice(transformed_50)\n",
    "C2_new = random.choice(transformed_50)\n",
    "while np.mean(C1_new - C2_new) == 0.0:\n",
    "    C2_new = random.choice(transformed_50) #prevent same cluster centres from being chosen for both C1 and C2\n",
    "\n",
    "C1_old = np.zeros(len(C1_new))\n",
    "C2_old = np.zeros(len(C2_new))\n",
    "C1_new = np.array(C1_new)\n",
    "C1_new = np.array(C1_new)\n",
    "\n",
    "while np.mean(C1_new - C1_old) != 0.0 and np.mean(C2_new - C2_old) != 0.0:\n",
    "    cluster1 = [] #array for all elements in cluster1\n",
    "    cluster2 = [] #array for all elements in cluster2\n",
    "    cluster1.append(C1_new)\n",
    "    cluster2.append(C2_new)\n",
    "    for line in transformed_50:\n",
    "        distance1 = distance.euclidean(line,C1_new)\n",
    "        distance2 = distance.euclidean(line,C2_new)\n",
    "        if distance1 > distance2: #review belong to cluster centre 1\n",
    "            cluster1.append(line)\n",
    "        else: \n",
    "            cluster2.append(line)\n",
    "    C1_old = C1_new\n",
    "    C2_old = C2_new\n",
    "    C1_new = np.mean(cluster1, axis = 0)\n",
    "    C2_new = np.mean(cluster2, axis = 0)\n",
    "    \n",
    "##plotting histogram\n",
    "plt.plot(C1_new)\n",
    "plt.suptitle('Cluster1 - pca_50D', fontsize=20)\n",
    "plt.xlabel('element', fontsize=16)\n",
    "plt.ylabel('frequency', fontsize=16)\n",
    "plt . savefig (\"Cluster1 - pca_50D.png\") #saving graph as .png file\n",
    "plt.close()\n",
    "\n",
    "plt.plot(C2_new)\n",
    "plt.suptitle('Cluster2 - pca_50D', fontsize=20)\n",
    "plt.xlabel('element', fontsize=16)\n",
    "plt.ylabel('frequency', fontsize=16)\n",
    "plt . savefig (\"Cluster2 - pca_50D.png\") #saving graph as .png file\n",
    "plt.close()\n",
    "\n",
    "#finding Classification Accuracy\n",
    "kTrainLabel_50 = []\n",
    "\n",
    "for line in transformed_50:\n",
    "    distance1 = distance.euclidean(line,C1_new)\n",
    "    distance2 = distance.euclidean(line,C2_new)\n",
    "    if distance1 < distance2:\n",
    "        kTrainLabel_50.append(\"0\")\n",
    "    else:\n",
    "        kTrainLabel_50.append(\"1\")\n",
    "\n",
    "kTrainLabel_50= np.array(kTrainLabel_50)\n",
    "trainLabel = np.array(trainLabel)\n",
    "accuracy = np.sum(kTrainLabel_50 == trainLabel)/float(len(trainLabel))\n",
    "\n",
    "print (\"K-Means Clustering classification accuracy for 50-D: %0.2f\" %accuracy)\n",
    "\n",
    "#(g) - Logistic Regression\n",
    "        \n",
    "#performing Logistic Regression\n",
    "model = LogisticRegression()\n",
    "transformed_50 = np.array(transformed_50)\n",
    "transformed_test_50 = np.array(transformed_test_50)\n",
    "testLabel = np.array(testLabel)\n",
    "\n",
    "#predicting classification accuracy\n",
    "model.fit(transformed_50,trainLabel)\n",
    "predicted = model.predict(transformed_test_50)\n",
    "accuracy = np.sum(predicted == testLabel) / float(len(testLabel))\n",
    "\n",
    "#constructing confusion matrix\n",
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "\n",
    "for i in range(len(testLabel)):\n",
    "    if predicted[i] == \"0\" and testLabel[i] == \"0\":\n",
    "        TN += 1\n",
    "    elif predicted[i] == \"0\" and testLabel[i] == \"1\":\n",
    "        FN += 1\n",
    "    elif predicted[i] == \"1\" and testLabel[i] == \"1\":\n",
    "        TP += 1\n",
    "    else:\n",
    "        FP += 1\n",
    "        \n",
    "confMatrix = [[TP, FN], [FP, TN]]\n",
    "\n",
    "print (\"Logistic Regression classification accuracy: %0.2f\" %accuracy)\n",
    "print confMatrix\n",
    "\n",
    "#return top 10 words which are most important\n",
    "array = []\n",
    "\n",
    "for i in range(10):\n",
    "    x = V[i,:]\n",
    "    x = x.tolist()\n",
    "    xIndex = x.index(max(x))\n",
    "    array.append(features[xIndex])\n",
    "                             \n",
    "for line in array:\n",
    "    print line\n",
    "\n",
    "#weightVector = model.coef_\n",
    "#weightVector = weightVector.tolist()\n",
    "\n",
    "#coefficient = heapq.nlargest(10, weightVector[0])\n",
    "#weightedWords = []\n",
    "\n",
    "#for line in coefficient:\n",
    "#    weightedWords.append(features[weightVector[0].index(line)])\n",
    "#for line in weightedWords:\n",
    "#    print line\n",
    "\n",
    "#(f) - K-means clustering - 100\n",
    "#initialise cluster centers\n",
    "C1_new = random.choice(transformed_100)\n",
    "C2_new = random.choice(transformed_100)\n",
    "while np.mean(C1_new - C2_new) == 0.0:\n",
    "    C2_new = random.choice(transformed_100) #prevent same cluster centres from being chosen for both C1 and C2\n",
    "\n",
    "C1_old = np.zeros(len(C1_new))\n",
    "C2_old = np.zeros(len(C2_new))\n",
    "C1_new = np.array(C1_new)\n",
    "C1_new = np.array(C1_new)\n",
    "\n",
    "while np.mean(C1_new - C1_old) != 0.0 and np.mean(C2_new - C2_old) != 0.0:\n",
    "    cluster1 = [] #array for all elements in cluster1\n",
    "    cluster2 = [] #array for all elements in cluster2\n",
    "    cluster1.append(C1_new)\n",
    "    cluster2.append(C2_new)\n",
    "    for line in transformed_100:\n",
    "        distance1 = distance.euclidean(line,C1_new)\n",
    "        distance2 = distance.euclidean(line,C2_new)\n",
    "        if distance1 > distance2: #review belong to cluster centre 1\n",
    "            cluster1.append(line)\n",
    "        else: \n",
    "            cluster2.append(line)\n",
    "    C1_old = C1_new\n",
    "    C2_old = C2_new\n",
    "    C1_new = np.mean(cluster1, axis = 0)\n",
    "    C2_new = np.mean(cluster2, axis = 0)\n",
    "    \n",
    "##plotting histogram\n",
    "plt.plot(C1_new)\n",
    "plt.suptitle('Cluster1 - pca_100D', fontsize=20)\n",
    "plt.xlabel('element', fontsize=16)\n",
    "plt.ylabel('frequency', fontsize=16)\n",
    "plt . savefig (\"Cluster1 - pca_100D.png\") #saving graph as .png file\n",
    "plt.close()\n",
    "\n",
    "plt.plot(C2_new)\n",
    "plt.suptitle('Cluster2 - pca_100D', fontsize=20)\n",
    "plt.xlabel('element', fontsize=16)\n",
    "plt.ylabel('frequency', fontsize=16)\n",
    "plt . savefig (\"Cluster2 - pca_100D.png\") #saving graph as .png file\n",
    "plt.close()\n",
    "\n",
    "#finding Classification Accuracy\n",
    "kTrainLabel_100 = []\n",
    "\n",
    "for line in transformed_100:\n",
    "    distance1 = distance.euclidean(line,C1_new)\n",
    "    distance2 = distance.euclidean(line,C2_new)\n",
    "    if distance1 < distance2:\n",
    "        kTrainLabel_100.append(\"0\")\n",
    "    else:\n",
    "        kTrainLabel_100.append(\"1\")\n",
    "\n",
    "kTrainLabel_100= np.array(kTrainLabel_100)\n",
    "trainLabel = np.array(trainLabel)\n",
    "accuracy = np.sum(kTrainLabel_100 == trainLabel)/float(len(trainLabel))\n",
    "\n",
    "print (\"K-Means Clustering classification accuracy for 100-D: %0.2f\" %accuracy)\n",
    "\n",
    "#(g) - Logistic Regression\n",
    "        \n",
    "#performing Logistic Regression\n",
    "model = LogisticRegression()\n",
    "transformed_100 = np.array(transformed_100)\n",
    "transformed_test_100 = np.array(transformed_test_100)\n",
    "testLabel = np.array(testLabel)\n",
    "\n",
    "#predicting classification accuracy\n",
    "model.fit(transformed_100,trainLabel)\n",
    "predicted = model.predict(transformed_test_100)\n",
    "accuracy = np.sum(predicted == testLabel) / float(len(testLabel))\n",
    "\n",
    "#constructing confusion matrix\n",
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "\n",
    "for i in range(len(testLabel)):\n",
    "    if predicted[i] == \"0\" and testLabel[i] == \"0\":\n",
    "        TN += 1\n",
    "    elif predicted[i] == \"0\" and testLabel[i] == \"1\":\n",
    "        FN += 1\n",
    "    elif predicted[i] == \"1\" and testLabel[i] == \"1\":\n",
    "        TP += 1\n",
    "    else:\n",
    "        FP += 1\n",
    "        \n",
    "confMatrix = [[TP, FN], [FP, TN]]\n",
    "\n",
    "print (\"Logistic Regression classification accuracy: %0.2f\" %accuracy)\n",
    "print confMatrix\n",
    "\n",
    "#return top 10 words which are most important\n",
    "array = []\n",
    "\n",
    "for i in range(10):\n",
    "    x = V[i,:]\n",
    "    x = x.tolist()\n",
    "    xIndex = x.index(max(x))\n",
    "    array.append(features[xIndex])\n",
    "                             \n",
    "for line in array:\n",
    "    print line\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
